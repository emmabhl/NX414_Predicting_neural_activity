{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27fe4e62-fda1-4377-ad2e-e97d213b2a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /opt/conda/lib/python3.7/site-packages (4.7.1)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (3.8.0)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.28.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.11.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.64.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from h5py) (1.21.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gdown h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf16f621-ec0d-41e7-95b1-a7218562f38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1s6caFNRpyR9m7ZM6XEv_e8mcXT3_PnHS\n",
      "From (redirected): https://drive.google.com/uc?id=1s6caFNRpyR9m7ZM6XEv_e8mcXT3_PnHS&confirm=t&uuid=c76db502-df83-40e0-9fc7-b9b7ce0e5c17\n",
      "To: /home/jupyter/IT_data.h5\n",
      "100%|██████████| 384M/384M [00:13<00:00, 29.4MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'IT_data.h5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import load_it_data, visualize_img\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gdown\n",
    "url = \"https://drive.google.com/file/d/1s6caFNRpyR9m7ZM6XEv_e8mcXT3_PnHS/view?usp=share_link\"\n",
    "output = \"IT_data.h5\"\n",
    "gdown.download(url, output, quiet=False, fuzzy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4c536",
   "metadata": {},
   "source": [
    "### Part 2: Predict the neural activity with the task-driven modeling approach\n",
    "\n",
    "As you have seen in the class, the underlying hypothesis of task-driven modeling is that training the network to perform a relevant behavioral task makes the network to develop representations that resemble the ones of the biological brain. Let's test this hypothesis by loading a pre-trained ResNet50 model and use the activations of each layer to predict the neural activity. Follow these steps:\n",
    "\n",
    "- Give as input to the network the stimuli and extract the corresponding activations of the following layers ['conv1','layer1','layer2','layer3','layer4','avgpool']\n",
    "- Compute the 1000 PCs for each layer activation. (Careful that you don't want to store all activations together at the same time because it won't fit in the memory. Therefore, compute the activations and corresponding PCs for each layer and store only the computed PCs).\n",
    "- Use the PCs of each layer to predict the neural activity using the linear regression models you developed before.\n",
    "- Compute the goodness of fit using the correlation and explained variance metrics. Do you predict the neural activity better than before?\n",
    "- Plot the distribution of explained variance with respect to the layer of the network (order them based on the depth). How does the neural activity changes across the model layers, can you make some statements about it?\n",
    "- Compare the predictions that you obtained using one layer of the pretrained model and the one obtained using the same layer but from a randomly initialized model. Which network can better predict the neural activity and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7a034ae-3770-40f8-b9f6-ddb2769db816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_it_data, visualize_img\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import explained_variance_score, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torchvision.models import ResNet, resnet50, ResNet50_Weights\n",
    "import pickle\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d79b99b3-fc02-4c70-bd00-023be24537d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '' ## Insert the folder where the data is, if you download in the same folder as this notebook then leave it blank\n",
    "\n",
    "stimulus_train, stimulus_val, stimulus_test, objects_train, objects_val, objects_test, spikes_train, spikes_val = load_it_data(path_to_data)\n",
    "layers = [\"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\", \"avgpool\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5c4f6cd-e7cb-4018-a364-84f9393cd7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_PCA(activations, n_components = 1000) :\n",
    "    \"\"\"apply PCA on the activations of a layer\n",
    "\n",
    "    Args:\n",
    "        layer_file (string): name of the layer where the activations data are extracted\n",
    "        n_components (int): number of components we want to keep\n",
    "\n",
    "    Returns:\n",
    "        activations: computed PC from the activations\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(activations)\n",
    "    activations = pca.transform(activations)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898f716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_activations(self, stimuli: Tensor, layer) :\n",
    "    \"\"\"extract the activations of the model for the given stimuli and layer\n",
    "\n",
    "    Args:\n",
    "        model (model): model we want to extract the activations from\n",
    "        stimuli (ndarray): input data of the processed image's pixels\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary containing the activations for each layer of the model\n",
    "    \"\"\"    \n",
    "    activations = []\n",
    "    for x in tqdm(stimuli) : \n",
    "        x = self.conv1(x.unsqueeze(0))\n",
    "        if layer == 'conv1' : \n",
    "            activations.append(torch.flatten(x.squeeze(0)).detach().numpy())\n",
    "            continue\n",
    "        \n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        if layer == 'layer1' : \n",
    "            activations.append(torch.flatten(x.squeeze(0)).detach().numpy())\n",
    "            continue\n",
    "    \n",
    "        x = self.layer2(x)\n",
    "        if layer == 'layer2' : \n",
    "            activations.append(torch.flatten(x.squeeze(0)).detach().numpy())\n",
    "            continue\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        if layer == 'layer3' : \n",
    "            activations.append(torch.flatten(x.squeeze(0)).detach().numpy())\n",
    "            continue\n",
    "    \n",
    "        x = self.layer4(x)\n",
    "        if layer == 'layer4' :       \n",
    "            activations.append(torch.flatten(x.squeeze(0)).detach().numpy())\n",
    "            continue\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        if layer == 'avgpool' : \n",
    "            activations.append(torch.flatten(x.squeeze(0)).detach().numpy())\n",
    "            continue\n",
    "    return activations\n",
    "\n",
    "ResNet.extract_activations = extract_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ed9e687-92aa-4fd6-ae49-cfb77bc8a328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the pre-trained ResNet50 model\n",
    "stimulus_train = torch.tensor(stimulus_train)\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT) # include_top = False?\n",
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f3aba99-dba9-4912-9a60-f567e43fc5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preprocess = weights.transforms()\\nimg_transformed = preprocess(stimuli)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocess the stimuli (already done)\n",
    "\"\"\"preprocess = weights.transforms()\n",
    "img_transformed = preprocess(stimuli)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f8081d9-d215-43b8-b67d-b4af94cf02e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2592/2592 [00:01<00:00, 1548.77it/s]\n",
      "  0%|          | 0/2592 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_3207/368174715.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# extract the activations of the layers and apply PCA on each layer to store the first 1000PCs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstimulus_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/tmp/ipykernel_3207/4014807184.py\u001b[0m in \u001b[0;36mextract_activation\u001b[0;34m(self, stimuli)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "# extract the activations of the layers and apply PCA on each layer to store the first 1000PCs\n",
    "layers = [\"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\", \"avgpool\"]\n",
    "for layer in layers : \n",
    "    with open(layer+'.pkl','wb') as f:\n",
    "        pickle.dump(apply_PCA(model.extract_activations(stimulus_train, layer)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d91ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(model, dataset, true_prediction) : \n",
    "    \"\"\"compute the r2 correlation coefficient and explained variance for each neuron individually\n",
    "\n",
    "    Args:\n",
    "        model : ML model we want to evaluate\n",
    "        dataset (ndarray): input data of the processed image's pixels. Used to compute the prediction\n",
    "        true_prediction (_type_): actual neural activity of the neuron\n",
    "\n",
    "    Returns:\n",
    "        tuple : r2 correlation coefficient and explained variance arrays for each neuron\n",
    "    \"\"\"\n",
    "    predict = model.predict(dataset)\n",
    "    r2 = r2_score(true_prediction, predict, multioutput = 'raw_values')\n",
    "    var = explained_variance_score(true_prediction, predict, multioutput = 'raw_values')\n",
    "    return r2, var\n",
    "\n",
    "\n",
    "def scores_plot(r2, var) : \n",
    "    \"\"\"plot the r2 correlation coefficient and explained variance for each neuron individually\n",
    "\n",
    "    Args:\n",
    "        r2 (array): r2 coefficient computed for a given model\n",
    "        var (array): explained variance computed for a given model\n",
    "\n",
    "    Returns:\n",
    "        tuple ([n0, n1, ...], bins, [patches0, patches1, ...]): scatter plot of the scores\n",
    "    \"\"\"\n",
    "    p = plt.figure(figsize=(12,7))\n",
    "    plt.scatter(np.arange(len(r2)), r2, label = 'r2 score')\n",
    "    plt.scatter(np.arange(len(var)), var, label = 'explained variance')\n",
    "    plt.xlabel('IT neurons')\n",
    "    plt.ylabel('score')\n",
    "    plt.legend()\n",
    "    return p\n",
    "\n",
    "def distribution_plot(var) :\n",
    "    \"\"\"histogram of the distribution of the explained variance across neurons\n",
    "\n",
    "    Args:\n",
    "        var (array of double): explained variance computed for each neuron for the model\n",
    "\n",
    "    Returns:\n",
    "        tuple ([n0, n1, ...], bins, [patches0, patches1, ...]): histogram of the distribution\n",
    "    \"\"\"\n",
    "    p = plt.figure(figsize=(12,7))\n",
    "    plt.hist(var, bins=20)\n",
    "    plt.xlabel('explained variance')\n",
    "    plt.ylabel('count')\n",
    "    return p\n",
    "\n",
    "def evaluate_prediction_plot(model, dataset, true_prediction, model_title=None) :\n",
    "    \"\"\"compute and plot the r2 coefficient and explained variance for each neuron and the distribution of the explained variance\n",
    "\n",
    "    Args:\n",
    "        model : ML model we want to evaluate\n",
    "        dataset (ndarray): input data of the processed image's pixels. Used to compute the prediction\n",
    "        true_prediction (array): actual neural activity of the neuron \n",
    "        model_title (string, optional): description of the model used, to add at the end of the title of the graphs. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    r2, var = compute_scores(model, dataset, true_prediction)\n",
    "    \n",
    "    scatter_plot = scores_plot(r2, var)\n",
    "    if model_title is None:\n",
    "        scatter_plot.suptitle('R2 score and explained variance on the validation set for each IT neurons')\n",
    "    else:\n",
    "        scatter_plot.suptitle('R2 score and explained variance on the validation set for each IT neurons when fitting a {}'.format(model_title))\n",
    "    \n",
    "    dist_plot = distribution_plot(var)\n",
    "    if model_title is None:\n",
    "        dist_plot.suptitle('Histogram of the explained variance across neurons')\n",
    "    else :\n",
    "        dist_plot.suptitle('Histogram of the explained variance across neurons when fitting a {}'.format(model_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078f6b4-8339-4cce-980e-6ae1f7b92fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the PCs of each layer to predict the neural activity using linear regression models\n",
    "models = {}\n",
    "for layer in tqdm(layers) : \n",
    "    with open(layer+'.pkl', 'rb') as f:\n",
    "        activations = pickle.load(f)  \n",
    "    model = LinearRegression()\n",
    "    model.fit(activations, spikes_train)\n",
    "    models[layer] = model\n",
    "\n",
    "# evaluate the models on the validation set\n",
    "for layer in layers :\n",
    "    evaluate_prediction_plot(models[layer], models[layer].predict(flat_val), spikes_val, model_title='linear regression on the activations of the {} layer'.format(layer))\n",
    "plt.show()\n",
    "    \n",
    "# Compute the goodness of fit using the correlation and explained variance metrics. Do you predict the neural activity better than before?\n",
    "# Plot the distribution of explained variance with respect to the layer of the network (order them based on the depth). How does the neural activity changes across the model layers, can you make some statements about it?\n",
    "# Compare the predictions that you obtained using one layer of the pretrained model and the one obtained using the same layer but from a randomly initialized model. Which network can better predict the neural activity and why?"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m107"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
