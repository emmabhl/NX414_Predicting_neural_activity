{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NX-414: Brain-like computation and intelligence\n",
    "##### TA: Alessandro Marin Vargas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 - Mini projects (Predicting neural activity)\n",
    "\n",
    "The objectives of the mini project are:\n",
    "- Learn how to predict neural activity using linear regression from images and from neural network layers.\n",
    "- Quantify the goodness of the model\n",
    "- Compare the results across the network layers and between trained/random neural network\n",
    "- Predict the neural activity using a neural network in a data-driven approach\n",
    "\n",
    "Specifically, here you will use the data from the following [paper](https://www.jneurosci.org/content/jneuro/35/39/13402.full.pdf). The behavioral experiment consisted in showing to non-human primates some images while recording the neural activity with multielectrode arrays from the inferior temporal (IT) cortex. Here, the neural activity and the images are already pre-processed and you will have available the images and the corresponding average firing rate (between 70 and 170 ms) per each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /opt/conda/lib/python3.7/site-packages (4.7.1)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (3.8.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.64.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.11.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from h5py) (1.21.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./../')\n",
    "!{sys.executable} -m pip install gdown h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1s6caFNRpyR9m7ZM6XEv_e8mcXT3_PnHS\n",
      "From (redirected): https://drive.google.com/uc?id=1s6caFNRpyR9m7ZM6XEv_e8mcXT3_PnHS&confirm=t&uuid=227ff9da-8f8f-4026-9e30-041aebe04e03\n",
      "To: /home/jupyter/IT_data.h5\n",
      "100%|██████████| 384M/384M [00:07<00:00, 53.9MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'IT_data.h5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import load_it_data\n",
    "import gdown\n",
    "url = \"https://drive.google.com/file/d/1s6caFNRpyR9m7ZM6XEv_e8mcXT3_PnHS/view?usp=share_link\"\n",
    "output = \"IT_data.h5\"\n",
    "gdown.download(url, output, quiet=False, fuzzy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '' ## Insert the folder where the data is, if you download in the same folder as this notebook then leave it blank\n",
    "\n",
    "stimulus_train, stimulus_val, stimulus_test, objects_train, objects_val, objects_test, spikes_train, spikes_val = load_it_data(path_to_data)\n",
    "layers = [\"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\", \"avgpool\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Develop the most accurate model for predicting IT neural activity.\n",
    "\n",
    "Throughout the mini-projects, you have acquired knowledge on predicting neural activity through both task-driven and data-driven approaches. You are now free to explore both approaches to create the most effective model for predicting neural activity. Here are some suggestions to help guide your exploration:\n",
    "- Optimize your current models by adjusting hyperparameters and implementing different regularizations.\n",
    "- Utilize different pretrained models.\n",
    "- Employ a pretrained neural network and finetune it for predicting the neural activity.\n",
    "- Train a neural network for object recognition while simultaneously predicting neural activity.\n",
    "- ...\n",
    "\n",
    "Please note that all models will be tested using the same type of linear regression.\n",
    "\n",
    "### EVALUATION\n",
    "\n",
    "You have until midnight of 19/04/23 to submit your mini-projects. Please upload a .zip file on Moodle containing your code and, specifically, the following:\n",
    "\n",
    "- A one-page report (Report.pdf) detailing your investigation, including figures and model comparisons.\n",
    "- A script (test.py) or notebook (test.ipynb) in which you show an example of your best model's usage. Please include instructions on loading the model and making neural predictions on the validation set. If necessary, include the checkpoint of your model in the .zip file.\n",
    "\n",
    "Best of luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import explained_variance_score, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import ResNet, resnet50, ResNet50_Weights\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune ResNet50 for classification of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    val_acc_history = []\n",
    "    model = model.cuda()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            # Initialize the losses for this epoch\n",
    "            loss = 0.0\n",
    "            corrects = 0\n",
    "            \n",
    "            steps_per_epoch_train = len(dataloaders['train'])\n",
    "            steps_per_epoch_val = len(dataloaders['val'])\n",
    "            \n",
    "            for stimulus, label in dataloaders[phase]:\n",
    "                stimulus, label = stimulus.cuda(), label.cuda()\n",
    "\n",
    "                # zero out the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Forward pass (compute training loss)\n",
    "                    prediction = model(stimulus)\n",
    "                    loss = criterion(prediction, label)\n",
    "                    \n",
    "                    _, preds = torch.max(prediction, 1)\n",
    "\n",
    "                    # Backward pass (compute new gradients, update weights))\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Update the loss for this epoch\n",
    "                loss += loss.item() * stimulus.size(0)\n",
    "                corrects += torch.sum(preds == label.data)\n",
    "\n",
    "            epoch_loss = loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "    \n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_classes = len(np.unique(objects_train))\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 50\n",
    "LR = 0.001 \n",
    "\n",
    "feature_extract = False # to finetune the model\n",
    "\n",
    "# Datasets and dataloaders\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "dataset_train = TensorDataset(torch.tensor(stimulus_train), torch.tensor(le.fit_transform(objects_train)))\n",
    "dataset_val = TensorDataset(torch.tensor(stimulus_val), torch.tensor(le.fit_transform(objects_val)))\n",
    "dataset_test = TensorDataset(torch.tensor(stimulus_test))\n",
    "dataloaders = {'train' : DataLoader(dataset_train, shuffle=True, batch_size=BATCH_SIZE), \n",
    "               'val' : DataLoader(dataset_val, batch_size=BATCH_SIZE)}\n",
    "testDataLoader = DataLoader(dataset_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and reshape the model\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "#model.fc = nn.Linear(512, num_classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "params_to_update = model.parameters()\n",
    "\n",
    "# Create the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params_to_update, lr=LR, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 0.0041 Acc: 0.1593\n",
      "val Loss: 0.0857 Acc: 0.4028\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.5544\n",
      "val Loss: 0.0250 Acc: 0.7361\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 0.7832\n",
      "val Loss: 0.0069 Acc: 0.8542\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.0017 Acc: 0.8843\n",
      "val Loss: 0.0049 Acc: 0.8750\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 0.9259\n",
      "val Loss: 0.0053 Acc: 0.8472\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.0015 Acc: 0.9483\n",
      "val Loss: 0.0030 Acc: 0.8785\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 0.9672\n",
      "val Loss: 0.0036 Acc: 0.9097\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 0.9880\n",
      "val Loss: 0.0040 Acc: 0.9271\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 0.9931\n",
      "val Loss: 0.0023 Acc: 0.9201\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9931\n",
      "val Loss: 0.0132 Acc: 0.9271\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9965\n",
      "val Loss: 0.0045 Acc: 0.9271\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.0025 Acc: 0.9961\n",
      "val Loss: 0.0112 Acc: 0.9236\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 0.9973\n",
      "val Loss: 0.0081 Acc: 0.9271\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9950\n",
      "val Loss: 0.0051 Acc: 0.9236\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 0.9981\n",
      "val Loss: 0.0028 Acc: 0.9340\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9977\n",
      "val Loss: 0.0008 Acc: 0.9375\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9988\n",
      "val Loss: 0.0012 Acc: 0.9340\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9988\n",
      "val Loss: 0.0052 Acc: 0.9132\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9985\n",
      "val Loss: 0.0013 Acc: 0.9340\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9996\n",
      "val Loss: 0.0019 Acc: 0.9375\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 1.0000\n",
      "val Loss: 0.0017 Acc: 0.9340\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 1.0000\n",
      "val Loss: 0.0025 Acc: 0.9375\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "model, _ = train_model(model, dataloaders, criterion, optimizer, num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the activations and predict the neural activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_activations(self, stimuli, layer) :\n",
    "    \"\"\"extract the activations of the model for the given stimuli and layer\n",
    "\n",
    "    Args:\n",
    "        model (model): model we want to extract the activations from\n",
    "        stimuli (ndarray): input data of the processed image's pixels\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary containing the activations for each layer of the model\n",
    "    \"\"\"    \n",
    "    stimuli = torch.tensor(stimuli)\n",
    "    activations = []\n",
    "    for x in stimuli : \n",
    "        x = self.conv1(x.unsqueeze(0))\n",
    "        if layer == 'conv1' : \n",
    "            activations.append(torch.flatten(x.squeeze(0)).detach().numpy())\n",
    "            continue\n",
    "        \n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        if layer == 'layer1' : \n",
    "            activations.append(torch.flatten(x.squeeze(0)).detach().numpy())\n",
    "            continue\n",
    "    \n",
    "        x = self.layer2(x)\n",
    "        if layer == 'layer2' : \n",
    "            activations.append(torch.flatten(x.squeeze(0)).detach().numpy())\n",
    "            continue\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        if layer == 'layer3' : \n",
    "            activations.append(torch.flatten(x.squeeze(0)).detach().numpy())\n",
    "            continue\n",
    "    \n",
    "        x = self.layer4(x)\n",
    "        if layer == 'layer4' :       \n",
    "            activations.append(torch.flatten(x.squeeze(0)).detach().numpy())\n",
    "            continue\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        if layer == 'avgpool' : \n",
    "            activations.append(torch.flatten(x.squeeze(0)).detach().numpy())\n",
    "            continue\n",
    "    return activations\n",
    "\n",
    "ResNet.extract_activations = extract_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the activations of the layers and apply PCA on each layer to store the first 1000PCs\n",
    "model.eval()\n",
    "for layer in tqdm(layers) : \n",
    "    activations_train = model.extract_activations(stimulus_train, layer)\n",
    "    pca = PCA(n_components=1000)\n",
    "    pca.fit(activations_train)    \n",
    "    with open(layer+'_train.pkl','wb') as f:\n",
    "        pickle.dump(pca.transform(activations_train), f)\n",
    "    with open(layer+'_val.pkl','wb') as f:\n",
    "        pickle.dump(pca.transform(model.extract_activations(stimulus_val, layer)), f)\n",
    "    with open(layer+'_test.pkl','wb') as f:\n",
    "        pickle.dump(pca.transform(model.extract_activations(stimulus_test, layer)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_alpha_Ridge(X, y, alphas):\n",
    "    \"\"\"implement cross validation to find the best alpha for Ridge regression\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): input data\n",
    "        y (ndarray): output data, neuronal activity\n",
    "        alphas (list of double): list of alpha to test\n",
    "\n",
    "    Returns:\n",
    "        tuple (double, ndarray): best alpha and all the scores for each alpha\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for alpha in alphas:\n",
    "        model = Ridge(alpha=alpha)\n",
    "        cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "        scores.append(np.mean(cv_scores))\n",
    "    return alphas[np.argmax(scores)], scores\n",
    "\n",
    "def plot_RidgeCV(alphas, scores):\n",
    "    \"\"\"plot the scores for each alpha\n",
    "\n",
    "    Args:\n",
    "        alphas (list of double): list of alpha that were tested\n",
    "        scores (list of double): list of scores for each alpha\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(3,2))\n",
    "    plt.plot(alphas, scores)\n",
    "    plt.xlabel('alpha')\n",
    "    plt.ylabel('r2 score')\n",
    "    plt.show()\n",
    "    \n",
    "def RidgeCV(X, y, alphas):\n",
    "    \"\"\"find the best alpha for Ridge regression and plot the scores for each alpha, then fit the model with the best alpha\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): input data\n",
    "        y (ndarray): output data, neuronal activity\n",
    "        alphas (list of double): list of alpha to test\n",
    "\n",
    "    Returns:\n",
    "        tuple (model, double): the ridge model fitted with the best alpha and the corresponding alpha\n",
    "    \"\"\"\n",
    "    best_alpha, scores = best_alpha_Ridge(X, y, alphas)\n",
    "    plot_RidgeCV(alphas, scores)\n",
    "    print('The best alpha is', best_alpha)\n",
    "    model = Ridge(alpha=best_alpha)\n",
    "    model.fit(X, y)\n",
    "    return model, best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the PCs of each layer to predict the neural activity using linear regression models\n",
    "models = {}\n",
    "best_alphas = {}\n",
    "alphas = {'conv1' : [11000000, 11500000, 12000000, 12500000, 13000000, 13500000, 14000000], \n",
    "          'layer1' : [750000, 800000, 850000, 900000, 950000, 1000000], \n",
    "          'layer2' : [300000, 310000, 320000, 330000, 340000, 350000, 360000, 370000, 380000, 390000, 400000], \n",
    "          'layer3' : [100000, 105000, 110000, 115000, 120000, 125000, 130000, 135000, 140000], \n",
    "          'layer4' : [80000, 85000, 90000, 95000, 100000, 105000, 110000], \n",
    "          'avgpool' : [180, 190, 200, 210, 220, 230, 240, 250, 260, 270]}\n",
    "\n",
    "for layer in layers : \n",
    "    with open(layer+'_train.pkl', 'rb') as f:\n",
    "        activations_train = pickle.load(f)  \n",
    "    print('\\n\\nR2 score for', layer, 'in function of the parameter alpha : ')\n",
    "    models[layer], best_alphas[layer] = RidgeCV(activations_train, spikes_train, alphas[layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models on the validation set\n",
    "import pandas as pd\n",
    "var = pd.DataFrame()\n",
    "for layer in layers : \n",
    "    with open(layer+'_val.pkl', 'rb') as f:\n",
    "        activations_val = pickle.load(f)  \n",
    "    var[layer] = explained_variance_score(spikes_val, models[layer].predict(activations_val), multioutput = 'raw_values')\n",
    "\n",
    "var.plot(kind='box', title='Boxplot of the explained variance for the different IT neurons for each layer', ylabel='explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with layer3 activation of a randomly initialized resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize model\n",
    "model_scratch = resnet50(weights = None)\n",
    "\n",
    "# Extract the activations and compute the PCs\n",
    "activations_train = model_scratch.extract_activations(stimulus_train, 'layer3')\n",
    "pca = PCA(n_components=1000)\n",
    "pca.fit(activations_train)    \n",
    "with open('scratch_train.pkl','wb') as f:\n",
    "    pickle.dump(pca.transform(activations_train), f)\n",
    "with open('scratch_val.pkl','wb') as f:\n",
    "    pickle.dump(pca.transform(model_scratch.extract_activations(stimulus_val, layer)), f)\n",
    "    \n",
    "# Predict the neural activity\n",
    "alphas = [100000, 105000, 110000, 115000, 120000, 125000, 130000, 135000, 140000]\n",
    "with open('scratch_train.pkl', 'rb') as f:\n",
    "    activations_train = pickle.load(f)  \n",
    "print('\\n\\nR2 score for the layer 3 of the scratch model in function of the parameter alpha : ')\n",
    "ridge_scratch, best_alphas_scratch = RidgeCV(activations_train, spikes_train, alphas)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "with open('scratch_val.pkl', 'rb') as f:\n",
    "    activations_val = pickle.load(f)  \n",
    "var['scratch layer 3'] = explained_variance_score(spikes_val, ridge_scratch.predict(activations_val), multioutput = 'raw_values')\n",
    "    \n",
    "var.plot(kind='box', title='Boxplot of the explained variance for the different IT neurons for each layer', ylabel='explained variance')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m107"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "698955d2a440f09c139f7b7d2bd7d8c99823f6917bcec6f9238f0f39f5a39694"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
